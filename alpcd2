b) A CLI dever ́a permitir ao utilizador, a partir da lista de trabalhos, criar uma contagem de vagas por tipo/nome da posi ̧c ̃ao e por regi ̃ao.
Deve produzir um ficheiro CSV com as seguintes colunas:
• Zona;
• Tipo de Trabalho; • No de vagas.
Exemplo :
> python jobs.py statistics zone
> Ficheiro de exporta ̧c~ao criado com sucesso.

c) A CLI dever ́a permitir ao utilizador, a partir da lista de trabalhos no textitwebsite (ambitionbox), recolher as principais skills (top10) pedidas para um determinado trabalho. Para cada skill encontrada, deve apresentar o seu nome e a contagem de ocorrˆencias. Por exemplo para data scientist, a url utilizando o website seria: https://www.ambitionbox.com/jobs/search?tag=data%20scientist.
Deve imprimir a informa ̧c ̃ao no terminal em formato JSON. Exemplo :
         > python jobs.py list skills "data scientist"
         > [{skill:"python", count:1000},...] --- csv 

d) Explore pelo menos uma fonte de dados alternativa (website) que poderia ser adicionada para recolher as in- forma ̧c ̃oes pedidas acima.
Justifique porque a escolheu e fa ̧ca a implementa ̧c ̃ao de pelos menos a al ́ınea a).


import typer
import requests
import json
import csv
import re
from datetime import datetime
from typing import Optional, List
from bs4 import BeautifulSoup

app = typer.Typer()

URL = "https://api.itjobs.pt/job/list.json?api_key=7948cb253161333d9cd4634fa957cc86"
HEADERS= {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0"
}
    
# html
def limpar_html(texto_html: str) -> str:
    texto_limpo = re.sub(r"<[^>]+>", "", texto_html)
    return texto_limpo.strip()

# exportar CSV
def export_to_csv(data: List[dict], filename: str):
    try:
        with open(filename, mode="w", newline="", encoding="utf-8") as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=["id", "titulo", "empresa", "descrição", "data de publicação", "salário", "localização"])
            writer.writeheader()
            for job in data:
                writer.writerow(job)
        print(f"CSV '{filename}' criado com sucesso!")
    except Exception as e:
        print(f"Erro ao criar o CSV: {e}")

@app.command()
def listar_n_trabalhos(n: int, export_csv: Optional[str] = None):
    response = requests.get(URL, headers=HEADERS)
    jobs = response.json().get("results", [])
    
    # Ordenar por data de pub
    ordenados = sorted(jobs, key=lambda job: datetime.strptime(job["publishedAt"], "%Y-%m-%d %H:%M:%S"), reverse=True)
    mais_recentes = ordenados[:n]
    
    n_recentes = []
    for job in mais_recentes:
        n_recentes.append({
            "id": job.get("id", "N/A"),
            "titulo": job.get("title", "N/A"),
            "empresa": job.get("company", {}).get("name", "N/A"),
            "descrição": limpar_html(job.get("body", "N/A")),
            "data de publicação": job.get("publishedAt", "N/A"),
            "salário": job.get("wage", "N/A"),
            "localização": "; ".join(loc.get("name", "N/A") for loc in job.get("locations", []))
        })
    
    # terminal 
    print("Trabalhos mais recentes:")
    for job in mais_recentes:
        print(f"- {job.get('title', 'N/A')} (Publicado em: {job.get('publishedAt', 'N/A')})")
    
    # CSV
    if export_csv:
        export_to_csv(n_recentes, export_csv)


@app.command()
def full_time_emp(company: str, location: str, limit: int, export_csv: Optional[str] = None):
    response = requests.get(URL, headers=HEADERS)
    jobs = response.json().get("results", [])

    company = company.lower()
    location = location.lower()
    filtered_jobs = []

    for job in jobs:
        if job.get("company", {}).get("name", "").lower() == company:
            if any(t["name"] == "Full-time" for t in job.get("types", [])) and \
               any(l["name"].lower() == location for l in job.get("locations", [])):
                filtered_jobs.append({
                    "id": job["id"],
                    "titulo": job["title"],
                    "empresa": job["company"]["name"],
                    "descrição": limpar_html(job["body"]),
                    "data de publicação": job["publishedAt"],
                    "salário": job.get("wage", "N/A"),
                    "localização": "; ".join(loc["name"] for loc in job.get("locations", []))
                })

    # limite
    filtered_jobs = filtered_jobs[:limit]
     
    if not filtered_jobs:
        typer.echo(f"Não foram encontrados empregos do tipo 'Full-time' em '{location}' para a empresa '{company}'.")
    else:
        print(json.dumps(filtered_jobs, indent=2, ensure_ascii=False))

    # CSV
    if export_csv:
        export_to_csv(filtered_jobs, export_csv)

@app.command()
def salary(job_id: int):
    response = requests.get(URL, headers=HEADERS)
    jobs = response.json().get("results", [])
    job = next((j for j in jobs if j["id"] == job_id), None)

    if not job:
        typer.echo(f"Job ID {job_id} não encontrado.")
        raise typer.Exit()
    # salary no wage 
    wage = job.get("wage")
    if wage:
        typer.echo(f"Salário encontrado: {wage}")
    else:
        # salary no body
        body = job.get("body", "")
        salaries = re.findall(r"(\€|\$|USD|EUR)?\s?\d+[,\.\d]*\s?(k|mil|m)?\s?(\€|\$|USD|EUR)?", body, re.IGNORECASE)
        if salaries:
            typer.echo(f"Salário(s) encontrado(s): {', '.join(' '.join(sal) for sal in salaries)}")
        else:
            typer.echo("Salário não encontrado em nenhum campo.")

@app.command()
def skills(skill: List[str], datainicial: str, datafinal: str, export_csv: Optional[str] = None):
    response = requests.get(URL, headers=HEADERS)
    jobs = response.json().get("results", [])
    
    datainicial = datetime.strptime(datainicial, "%Y-%m-%d")
    datafinal = datetime.strptime(datafinal, "%Y-%m-%d")
    filtered_jobs = []

    for job in jobs:
        published_at = datetime.strptime(job["publishedAt"].split(" ")[0], "%Y-%m-%d")
        if datainicial <= published_at <= datafinal:
            body_text = job.get("body", "").lower()
            if all(s.lower() in body_text for s in skill):
                filtered_jobs.append({
                    "id": job["id"],
                    "titulo": job["title"],
                    "empresa": job["company"]["name"],
                    "descrição": limpar_html(job["body"]),
                    "data de publicação": job["publishedAt"],
                    "salário": job.get("wage", "N/A"),
                    "localização": "; ".join(loc["name"] for loc in job.get("locations", []))
                })

    print(json.dumps(filtered_jobs, indent=2, ensure_ascii=False))

    # CSV
    if export_csv:
        export_to_csv(filtered_jobs, export_csv)
 
def itjobs_data(job_id: int) -> dict:
    page = 1  
    while True:
        paginated_url = f"{URL}&page={page}"
        response = requests.get(paginated_url, headers=HEADERS)

        if response.status_code == 200:
            jobs = response.json().get("results", [])

            # Procurar o job com o ID correspondente na página atual
            job = next((j for j in jobs if j.get("id") == job_id), None)
            if job:
                return job

            # Se não houver mais resultados, sair do loop
            if not jobs:
                break

            # Incrementar a página 
            page += 1
        else:
            typer.echo(f"Erro ao conectar à API itjobs.pt. Status: {response.status_code}")
            raise typer.Exit()

    typer.echo(f"Job ID {job_id} não encontrado em nenhuma página.")
    raise typer.Exit()

def ambitionbox_data(company_name: str) -> dict:

    # Formatar o nome da empresa para o padrão da URL
    company_name_formatted = company_name.strip().lower().replace(" ", "-")
    url = f"https://www.ambitionbox.com/overview/{company_name_formatted}-overview"
    
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        typer.echo(f"Erro ao acessar AmbitionBox para '{company_name}'. Status: {response.status_code}")
        return {"rating": "N/A", "description": "N/A", "benefits": "N/A"}

    soup = BeautifulSoup(response.text, "html.parser")

    # rating da empresa
    rating_element = soup.select_one("span.css-1jxf684.text-primary-text.font-pn-700.text-xl")
    rating = rating_element.text.strip() if rating_element else "N/A"

    # descrição da empresa
    description_element = soup.select_one("div[data-test='company-description']")
    description = description_element.text.strip() if description_element else "N/A"

    # benefícios da empresa
    benefits_element = soup.select_one("div[data-test='company-benefits']")
    benefits = benefits_element.text.strip() if benefits_element else "N/A"

    return {
        "rating": rating,
        "description": description,
        "benefits": benefits,
    }


@app.command()
def get_job_info(job_id: int, export_csv: Optional[str] = None):
    # API
    job_data = itjobs_data(job_id)
    company_name = job_data.get("company", {}).get("name", "").strip()
    if not company_name:
        typer.echo("Empresa não especificada para o emprego.")
        raise typer.Exit()

    # AmbitionBox
    ambitionbox_info = ambitionbox_data(company_name)

    # Combinar dados
    enriched_data = {
        "job_id": job_data.get("id"),
        "company_name": company_name,
        "rating": ambitionbox_info["rating"],
        "ambitionbox_description": ambitionbox_info["description"],
        "ambitionbox_benefits": ambitionbox_info["benefits"]
    }

    typer.echo(json.dumps(enriched_data, indent=2, ensure_ascii=False))

    #CSV
    if export_csv:
        with open(export_csv, mode="w", newline="", encoding="utf-8") as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=enriched_data.keys())
            writer.writeheader()
            writer.writerow(enriched_data)
        typer.echo(f"Informações exportadas para o arquivo '{export_csv}'.")

if __name__ == "__main__":
    app()
