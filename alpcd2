import typer
import requests
import json
import csv
import re
from datetime import datetime
from typing import Optional, List
from bs4 import BeautifulSoup
from typing import Annotated, Optional
from collections import defaultdict, Counter


app = typer.Typer()

URL = "https://api.itjobs.pt/job/list.json?api_key=7948cb253161333d9cd4634fa957cc86"
HEADERS ={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0", 
               "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8", 
               "Accept-Language": "en-US,en;q=0.5", 
               "Accept-Encoding": "gzip, deflate", 
               "Connection": "keep-alive", 
               "Upgrade-Insecure-Requests": "1", 
               "Sec-Fetch-Dest": "document", 
               "Sec-Fetch-Mode": "navigate", 
               "Sec-Fetch-Site": "none", 
               "Sec-Fetch-User": "?1", 
               "Cache-Control": "max-age=0"
         }
    
    
# html
def limpar_html(texto_html: str) -> str:
    texto_limpo = re.sub(r"<[^>]+>", "", texto_html)
    return texto_limpo.strip()

# exportar CSV
def export_to_csv(data: List[dict], filename: str):
    try:
        if not data:
            print("Nenhum dado para exportar.")
            return

        # Usar as chaves do primeiro item como fieldnames
        fieldnames = data[0].keys()

        with open(filename, mode="w", newline="", encoding="utf-8") as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)
        print(f"CSV '{filename}' criado com sucesso!")
    except Exception as e:
        print(f"Erro ao criar o CSV: {e}")





@app.command()
def listar_n_trabalhos(n: int, export_csv: Optional[str] = None):
    response = requests.get(URL, headers=HEADERS)
    jobs = response.json().get("results", [])
    
    # Ordenar por data de pub
    ordenados = sorted(jobs, key=lambda job: datetime.strptime(job["publishedAt"], "%Y-%m-%d %H:%M:%S"), reverse=True)
    mais_recentes = ordenados[:n]
    
    n_recentes = []
    for job in mais_recentes:
        n_recentes.append({
            "id": job.get("id", "N/A"),
            "titulo": job.get("title", "N/A"),
            "empresa": job.get("company", {}).get("name", "N/A"),
            "descrição": limpar_html(job.get("body", "N/A")),
            "data de publicação": job.get("publishedAt", "N/A"),
            "salário": job.get("wage", "N/A"),
            "localização": "; ".join(loc.get("name", "N/A") for loc in job.get("locations", []))
        })
    
    # terminal 
    print("Trabalhos mais recentes:")
    for job in mais_recentes:
        print(f"- {job.get('title', 'N/A')} (Publicado em: {job.get('publishedAt', 'N/A')})")
    
    # CSV
    if export_csv:
        export_to_csv(n_recentes, export_csv)


@app.command()
def full_time_emp(company: str, location: str, limit: int, export_csv: Optional[str] = None):
    response = requests.get(URL, headers=HEADERS)
    jobs = response.json().get("results", [])

    company = company.lower()
    location = location.lower()
    filtered_jobs = []

    for job in jobs:
        if job.get("company", {}).get("name", "").lower() == company:
            if any(t["name"] == "Full-time" for t in job.get("types", [])) and \
               any(l["name"].lower() == location for l in job.get("locations", [])):
                filtered_jobs.append({
                    "id": job["id"],
                    "titulo": job["title"],
                    "empresa": job["company"]["name"],
                    "descrição": limpar_html(job["body"]),
                    "data de publicação": job["publishedAt"],
                    "salário": job.get("wage", "N/A"),
                    "localização": "; ".join(loc["name"] for loc in job.get("locations", []))
                })

    # limite
    filtered_jobs = filtered_jobs[:limit]
     
    if not filtered_jobs:
        typer.echo(f"Não foram encontrados empregos do tipo 'Full-time' em '{location}' para a empresa '{company}'.")
    else:
        print(json.dumps(filtered_jobs, indent=2, ensure_ascii=False))

    # CSV
    if export_csv:
        export_to_csv(filtered_jobs, export_csv)

def extract_wage(job_data):
    # 'wage'
    wage = job_data.get("wage")
    if wage:
        return f"Salário encontrado no campo específico: {wage}"

    #  'body' usando expressões regulares
    body_text = job_data.get("body", "")
    salary_patterns = re.findall(r"(\€|\$|USD|EUR)?\s?\d+\s?(€|\$|USD|EUR)?", body_text, re.IGNORECASE)

    valid_salaries = [f"{pattern[0]}{pattern[1]}" if pattern[0] else f"{pattern[1]}" for pattern in salary_patterns if pattern[0] or pattern[1]]

    if valid_salaries:
        return f"Salário(s) encontrado(s) no campo 'body': {', '.join(valid_salaries)}"
    else:
        return "Salário não encontrado em nenhum campo."

@app.command()
def salary(job_id: int):
    response = requests.get(URL, headers=HEADERS)
    jobs = response.json().get("results", [])
        
    job_data = next((job for job in jobs if job["id"] == job_id), None)
    
    if job_data:
        wage_info = extract_wage(job_data)
        typer.echo(wage_info)
    else:
        typer.echo(f"Job ID {job_id} não encontrado.")
        raise typer.Exit()

@app.command()
def skills(skill: List[str], datainicial: str, datafinal: str, export_csv: Optional[str] = None):
    response = requests.get(URL, headers=HEADERS)
    jobs = response.json().get("results", [])
    
    datainicial = datetime.strptime(datainicial, "%Y-%m-%d")
    datafinal = datetime.strptime(datafinal, "%Y-%m-%d")
    filtered_jobs = []

    for job in jobs:
        published_at = datetime.strptime(job["publishedAt"].split(" ")[0], "%Y-%m-%d")
        if datainicial <= published_at <= datafinal:
            body_text = job.get("body", "").lower()
            if all(s.lower() in body_text for s in skill):
                filtered_jobs.append({
                    "id": job["id"],
                    "titulo": job["title"],
                    "empresa": job["company"]["name"],
                    "descrição": limpar_html(job["body"]),
                    "data de publicação": job["publishedAt"],
                    "salário": job.get("wage", "N/A"),
                    "localização": "; ".join(loc["name"] for loc in job.get("locations", []))
                })

    print(json.dumps(filtered_jobs, indent=2, ensure_ascii=False))

    # CSV
    if export_csv:
        export_to_csv(filtered_jobs, export_csv)
 
def itjobs_data(job_id: int) -> dict:
    page = 1  
    while True:
        paginated_url = f"{URL}&page={page}"
        response = requests.get(paginated_url, headers=HEADERS)

        if response.status_code == 200:
            jobs = response.json().get("results", [])

            # Procurar o job com o ID correspondente na página atual
            job = next((j for j in jobs if j.get("id") == job_id), None)
            if job:
                return job

            # Se não houver mais resultados, sair do loop
            if not jobs:
                break

            # Incrementar a página 
            page += 1
        else:
            typer.echo(f"Erro ao conectar à API itjobs.pt. Status: {response.status_code}")
            raise typer.Exit()

    typer.echo(f"Job ID {job_id} não encontrado em nenhuma página.")
    raise typer.Exit()

def ambitionbox_data(company_name: str) -> dict:

    # Formatar o nome da empresa para o padrão da URL
    company_name_formatted = company_name.strip().lower().replace(" ", "-")
    url = f"https://www.ambitionbox.com/overview/{company_name_formatted}-overview"
    
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200:
        typer.echo(f"Erro ao acessar AmbitionBox para '{company_name}'. Status: {response.status_code}")
        return {"rating": "N/A", "description": "N/A", "benefits": "N/A"}

    soup = BeautifulSoup(response.text, "html.parser")

    # rating da empresa
    rating_element = soup.select_one("span.css-1jxf684.text-primary-text.font-pn-700.text-xl")
    rating = rating_element.text.strip() if rating_element else "N/A"

    # descrição da empresa
    description_element = soup.select_one("div[data-test='company-description']")
    description = description_element.text.strip() if description_element else "N/A"

    # benefícios da empresa
    benefits_element = soup.select_one("div[data-test='company-benefits']")
    benefits = benefits_element.text.strip() if benefits_element else "N/A"

    return {
        "rating": rating,
        "description": description,
        "benefits": benefits,
    }


@app.command()
def get_job_info(job_id: int, export_csv: Optional[str] = None):
    # API
    job_data = itjobs_data(job_id)
    company_name = job_data.get("company", {}).get("name", "").strip()
    if not company_name:
        typer.echo("Empresa não especificada para o emprego.")
        raise typer.Exit()

    # AmbitionBox
    ambitionbox_info = ambitionbox_data(company_name)

    # Combinar dados
    enriched_data = {
        "job_id": job_data.get("id"),
        "company_name": company_name,
        "rating": ambitionbox_info["rating"],
        "ambitionbox_description": ambitionbox_info["description"],
        "ambitionbox_benefits": ambitionbox_info["benefits"]
    }

    typer.echo(json.dumps(enriched_data, indent=2, ensure_ascii=False))

    #CSV
    if export_csv:
        with open(export_csv, mode="w", newline="", encoding="utf-8") as csv_file:
            writer = csv.DictWriter(csv_file, fieldnames=enriched_data.keys())
            writer.writeheader()
            writer.writerow(enriched_data)
        typer.echo(f"Informações exportadas para o arquivo '{export_csv}'.")

@app.command()
def statistics():

    job_counts = {}
    page = 1
    has_more_results = True

    while has_more_results:
        dados = obter_dados(page)
        if dados is None or not dados.get("results"):
            has_more_results = False
            continue

        #  resultados
        for job in dados.get("results", []):
            title = job["title"]
            locations = ", ".join([loc["name"] for loc in job.get("locations", [])]) if job.get("locations") else "N/A"
            key = (title, locations)

            if key in job_counts:
                job_counts[key] += 1
            else:
                job_counts[key] = 1

        page += 1

    # exportação
    aggregated_data = [
        {"titulo": title, "localização": location, "número de vagas": count}
        for (title, location), count in job_counts.items()
    ]

    if aggregated_data:
        #CSV
        export_to_csv(aggregated_data, filename='numero_de_vagas.csv')
    else:
        print("Nenhum dado para exportar.")


def skills_from_job(job_url: str):
    response = requests.get(job_url, headers=HEADERS)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')
    skills_elements = soup.find_all('a', class_='body-medium chip') #skills
    skills = [skill.get_text(strip=True).lower() for skill in skills_elements] 
    return skills

def job_urls(job_title: str):
    job_title = job_title.replace(" ", "-") #limpar
    url = f"https://www.ambitionbox.com/jobs/{job_title}-jobs-prf"
    response = requests.get(url, headers=HEADERS)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, 'html.parser')
    job_elements = soup.find_all('div', class_='jobsInfoCardCont') #links dos jobs
    job_urls = [f"https://www.ambitionbox.com{job.find('a')['href']}" for job in job_elements] #urls dos jobs
    return job_urls

@app.command()
def list_skills(job_title: str, export_csv: Optional[str] = None):
    job_urls = job_urls(job_title)
    all_skills = []
    for url in job_urls: 
        skills = skills_from_job(url)
        all_skills.extend(skills)
    skill_count = Counter(all_skills)
    top_skills = skill_count.most_common(10)
    skills = [{"skill": skill, "count": count} for skill, count in top_skills] #ordena 
    print(json.dumps(skills, indent=4))
    if export_csv:
        exportar_csv(skills, filename=f'lista_skills_{job_title}.csv')
def indeed_data(job_id):
    company_name = find_company_name(job_id)
    if company_name is None:
        print(f"Empresa não encontrada para o job ID {job_id}.")
        return
    
    company_name1 = re.sub(r'\s+', '-', company_name)
    print(company_name1)
    url = f"https://pt.indeed.com/cmp/{company_name1}"
    
    response = requests.get(url, headers=HEADERS)
    if response.status_code != 200: 
        print(f"Website não encontrado para a empresa: {company_name}.")
        return None
    
    soup = BeautifulSoup(response.text, 'lxml')
    
    return soup

def indeed_data2(soup):
    if soup is None:
        return {
            "overall_rating": "N/A",
            "description": "Informações indisponíveis.",
        }

    body = soup.find('body')

    if not body:
        return {
            "overall_rating": "N/A",
            "description": "Informações indisponíveis.",
        }
    
    # encontrar o overall rating
    overall_rating = body.find('span', attrs={"aria-hidden":"true"}, class_="css-4f2law e1wnkr790").text  
    overall_rating_1 = overall_rating.text if overall_rating else "N/A"

    # encontrar a descrição da empresa
    description_div = body.find('div', attrs={"data-testid": "less-text", "class": "css-9146s eu4oa1w0"})     # em alguns casos é more_text, alterar
    if description_div:
        paragraphs = description_div.find_all('p')
        description = " ".join(p.text for p in paragraphs)
    else:
        description = "Informações indisponíveis."
    
    html_data = {
        "overall_rating": overall_rating_1,
        "description": description,        
    }

 return html_data

@app.command()
def get_job_indeed(job_id: int, export_csv: Optional[str] = None):
    api_data = get_api_content(job_id)
    if api_data is None:
        print(f"Job com ID {job_id} não encontrado na API.")
        return
    soup = get_html2(job_id)
    html_content = get_html_content2(soup)

    data = {**api_data, **html_content}
    print(json.dumps(data, indent=4))
    
    # exportar dados para csv (opcional)
    if export_csv:
        export_to_csv(data, export_csv)

if __name__ == "__main__":
    app()
